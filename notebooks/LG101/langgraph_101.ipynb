{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6e5cee",
   "metadata": {},
   "source": [
    "# LangGraph 101: Building Your First Agent (TypeScript)\n",
    "\n",
    "Welcome to LangGraph 101! This notebook will walk you through the core concepts of building agents with LangChain and LangGraph using TypeScript.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to interact with language models\n",
    "- Working with messages and conversation\n",
    "- Adding tools to extend LLM capabilities\n",
    "- Building an agent that can reason and act\n",
    "- Adding memory to maintain context\n",
    "- Streaming responses for better UX\n",
    "<br>\n",
    "<br>\n",
    "---\n",
    "<br>\n",
    "\n",
    "> **Note:** This tutorial uses LangChain v1, which provides the easiest way to start building with LLMs. LangChain agents are built on top of LangGraph, providing durable execution, streaming, human-in-the-loop, and persistence out of the box.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c09d828",
   "metadata": {},
   "source": [
    "## ðŸš€ Quick Start Checklist\n",
    "\n",
    "Before running this notebook, make sure you've completed these steps:\n",
    "\n",
    "1. âœ… Installed Node.js (v20+) and pnpm\n",
    "2. âœ… Run `pnpm install` from `/Users/victormoreira/Desktop/demos/langgraph-101-ts`\n",
    "3. âœ… Created a `.env` file with your `OPENAI_API_KEY`\n",
    "4. âœ… Installed tslab: `npm install -g tslab && tslab install`\n",
    "5. âœ… Selected \"TypeScript\" as your kernel (top-right in VS Code/Jupyter)\n",
    "\n",
    "**Tips:**\n",
    "- Wait 2-3 seconds between running cells\n",
    "- If you get errors, restart the kernel and try again\n",
    "- Full setup guide is in the [README.md](../../README.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d8668a",
   "metadata": {},
   "source": [
    "## Part 0: Setup & Installation\n",
    "\n",
    "First, let's install the necessary packages and set up our environment.\n",
    "\n",
    "> **âš ï¸ Important Notes:**\n",
    "> - **Before running this notebook**, make sure you've run `pnpm install` from the project root\n",
    "> - **Wait a few seconds** between running cells to avoid tslab timing issues\n",
    "> - If you get a \"rebuildTimer\" error, **restart the kernel** and try again\n",
    "> - See the README.md for full troubleshooting guide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f4e08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Make sure you've run 'pnpm install' from the project root before continuing!\n"
     ]
    }
   ],
   "source": [
    "// IMPORTANT: Before running this notebook, install dependencies from the project root:\n",
    "// \n",
    "// cd /Users/victormoreira/Desktop/demos/langgraph-101-ts\n",
    "// pnpm install\n",
    "// p\n",
    "// This will install: langchain, @langchain/core, @langchain/langgraph, \n",
    "// @langchain/openai, zod, uuid, and dotenv\n",
    "\n",
    "console.log(\"âœ“ Make sure you've run 'pnpm install' from the project root before continuing!\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10fa1d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Environment loaded successfully!\n",
      "\n",
      "ðŸ“ Make sure OPENAI_API_KEY is set in your .env file or environment\n"
     ]
    }
   ],
   "source": [
    "// Load environment variables\n",
    "try {\n",
    "    await import(\"dotenv/config\");\n",
    "    console.log(\"âœ“ Environment loaded successfully!\");\n",
    "} catch (error) {\n",
    "    console.log(\"âš ï¸  Could not load dotenv. Make sure you've run 'pnpm install' from the project root.\");\n",
    "    console.log(\"âš ï¸  You can continue if you've set OPENAI_API_KEY as a system environment variable.\");\n",
    "}\n",
    "\n",
    "// We'll use OpenAI in this tutorial, but you can swap to any provider!\n",
    "// Supported providers: OpenAI, Anthropic, Google, and many more\n",
    "console.log(\"\\nðŸ“ Make sure OPENAI_API_KEY is set in your .env file or environment\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d6f6c",
   "metadata": {},
   "source": [
    "## Part 1: Your First LLM Call\n",
    "\n",
    "LangChain provides a **standard model interface** that works across all providers. This means you can easily swap between OpenAI, Anthropic, Google, and other providers without changing your code.\n",
    "\n",
    "Let's start by initializing a chat model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74208dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is an open-source framework (works with Python and TypeScript/JavaScript) that helps you build applications powered by large language models (LLMs). It provides higher-level building blocks to orchestrate LLM calls, manage prompts and memory, integrate data sources, and even act autonomously using tools.\n",
      "\n",
      "Core concepts youâ€™ll often use in LangChain:\n",
      "- LLMs and Prompting: wrap LLM calls and define prompts (including templates and few-shot formats).\n",
      "- Chains: sequences of steps that combine prompts, LLMs, and other logic into a single workflow.\n",
      "- Memory: store and retrieve state across interactions (for more coherent chats or multi-turn tasks).\n",
      "- Agents and Tools: agents can decide which tools (APIs, databases, search, code execution, etc.) to use to accomplish a goal.\n",
      "- Tools: wrappers around external actions (e.g., web search, Python execution, database queries).\n",
      "- Vector stores: store and retrieve embeddings for retrieval-augmented generation (e.g., FAISS, Pinecone, Weaviate).\n",
      "- Data & evaluation: document loaders, parsers, and debugging/tracing utilities.\n",
      "\n",
      "Typical use cases:\n",
      "- Chatbots and virtual assistants with memory and tool use.\n",
      "- Question-answering with external data (retrieval-augmented generation).\n",
      "- Multi-step reasoning pipelines (summarization, analysis, and decision making).\n",
      "- Automating workflows that require LLM-generated results and actions via tools.\n",
      "\n",
      "Getting started (high level):\n",
      "- Install: pip install langchain (Python) or npm install langchain (TypeScript/JavaScript).\n",
      "- Create a simple prompt, wrap an LLM, and run a chain or a basic QA setup.\n",
      "- As you grow, add memory for context, connect a vector store for retrieval, and build an Agent to call tools.\n",
      "\n",
      "If youâ€™d like, I can provide a small code snippet to illustrate a simple LangChain chain (Python or TypeScript) or tailor the explanation to your intended use case.\n"
     ]
    }
   ],
   "source": [
    "import { initChatModel } from \"langchain\";\n",
    "\n",
    "// Initialize a chat model - you can easily swap providers!\n",
    "// Examples: \"openai:gpt-4o\", \"anthropic:claude-3-7-sonnet-latest\", \"google:gemini-2.0-flash\"\n",
    "const model = await initChatModel(\"openai:gpt-5-nano\");\n",
    "\n",
    "// Make your first call!\n",
    "const response = await model.invoke(\"What is LangChain?\");\n",
    "console.log(response.content);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf76ee",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- `initChatModel()` gives you a standardized interface to any LLM provider\n",
    "- `.invoke()` sends a message and returns a response\n",
    "- No provider lock-in - swap models easily!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7850a575",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Messages\n",
    "\n",
    "**Messages** are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both content and metadata.\n",
    "\n",
    "There are different message types:\n",
    "- **SystemMessage** - Instructions for how the model should behave\n",
    "- **HumanMessage** - User input\n",
    "- **AIMessage** - Model responses\n",
    "- **ToolMessage** - Results from tool executions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b48d1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An agent is something that can act on the world to achieve goals, based on what it perceives.\n",
      "\n",
      "In AI or robotics, an agent usually has:\n",
      "- Perception: sensors to detect whatâ€™s around it (camera, temperature, keyboard input, etc.)\n",
      "- Decision-making: a way to decide what to do next (rules, planning, learning)\n",
      "- Action: actuators or outputs to affect the world (move, turn on a motor, send a message)\n",
      "\n",
      "Other notes:\n",
      "- Autonomy: it can act without needing step-by-step instructions from a person.\n",
      "- Goals or value: it tries to maximize some objective (like staying at a comfortable temperature, delivering a message, or winning a game).\n",
      "\n",
      "Simple examples:\n",
      "- Thermostat: senses room temperature and turns the heater on or off to reach a desired temperature.\n",
      "- Cleaning robot: senses obstacles and dirt, plans a path, and moves around the room to clean.\n",
      "- Spam filter: senses incoming emails, decides if theyâ€™re spam, and moves them to a spam folder.\n",
      "\n",
      "Different contexts use the word â€œagentâ€:\n",
      "- Everyday life: a person or organization acting as a decision-maker.\n",
      "- AI/software: a program that perceives, reasons, and acts.\n",
      "- Economics: an economic agent is any decision-maker (a consumer, firm, etc.).\n",
      "\n",
      "If you have a specific context in mind (AI, robotics, software, etc.), I can tailor the explanation.\n"
     ]
    }
   ],
   "source": [
    "import { HumanMessage, SystemMessage } from \"langchain\";\n",
    "\n",
    "// Create a conversation with different message types\n",
    "const messages = [\n",
    "    new SystemMessage(\"You are a helpful AI assistant that explains technical concepts simply.\"),\n",
    "    new HumanMessage(\"What is an agent?\"),\n",
    "];\n",
    "\n",
    "const response2 = await model.invoke(messages);\n",
    "console.log(response2.content);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2115b2d",
   "metadata": {},
   "source": [
    "### Multi-turn Conversations\n",
    "\n",
    "Messages make it easy to maintain conversation history:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b40774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Autonomous household vacuum robot (a simple agent)\n",
      "\n",
      "- Perception: It uses sensors to detect walls and furniture (obstacles), dirt on the floor, and its own battery level.\n",
      "- Goal: Clean the whole floor efficiently while avoiding collisions and staying charged.\n",
      "- Decision-making: It decides where to go next (a cleaning path or area to explore), when to avoid an obstacle, and when to head back to the charging dock if the battery is low.\n",
      "- Action: It sends commands to its wheels to move, to the vacuum to start/stop cleaning, and to lights or beeps to indicate status.\n",
      "- Autonomy: It operates by itself without step-by-step instructions from a person (though you can override it or schedule tasks via an app).\n",
      "\n",
      "How a typical run might look:\n",
      "- It starts at the charging dock, senses room layout, and creates a cleaning plan.\n",
      "- It moves around, cleans dirtier spots more or less aggressively, and avoids collisions using obstacle sensors.\n",
      "- If a door is found or it detects a barrier, it reroutes its path.\n",
      "- If the battery drops to a low level, it navigates back to the dock to recharge, then resumes where it left off.\n"
     ]
    }
   ],
   "source": [
    "// Continue the conversation\n",
    "messages.push(response2);  // Add AI response to history\n",
    "messages.push(new HumanMessage(\"Can you give me an example?\"));\n",
    "\n",
    "const response3 = await model.invoke(messages);\n",
    "console.log(response3.content);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc5d969",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- Messages represent the conversation history\n",
    "- SystemMessage sets the model's behavior\n",
    "- Build multi-turn conversations by appending messages to an array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d7140",
   "metadata": {},
   "source": [
    "## Part 3: Adding Tools - Extending LLM Capabilities\n",
    "\n",
    "LLMs are great at language, but they can't access external data or perform actions. **Tools** extend their capabilities. You can give an LLM a list of tools, and when it needs one, it will specify which tool to call. Your job is to execute the tool and feed the results back to the LLM so it can decide what to do next.\n",
    "\n",
    "You can create a tool just by writing a function with a clear description. LangChain's `tool` function handles formatting the function's information in the LLM's desired format.\n",
    "\n",
    "Let's create some simple tools:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca7e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"temperature_fahrenheit\":66.1,\"weather_code\":0}\n"
     ]
    }
   ],
   "source": [
    "import { tool } from \"langchain\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "// Basic hardcoded tool\n",
    "const searchMovies = tool(\n",
    "  async ({ genre }: { genre: string }) => {\n",
    "    // In a real app, this would query a movie database\n",
    "    const movies: Record<string, string> = {\n",
    "      \"sci-fi\": \"Dune, Interstellar, Blade Runner 2049\",\n",
    "      \"comedy\": \"The Grand Budapest Hotel, Superbad, Knives Out\",\n",
    "      \"action\": \"Mad Max: Fury Road, John Wick, Mission Impossible\"\n",
    "    };\n",
    "    return movies[genre.toLowerCase()] || \"No movies found for that genre\";\n",
    "  },\n",
    "  {\n",
    "    name: \"search_movies\",\n",
    "    description: \"Search for movies by genre.\",\n",
    "    schema: z.object({\n",
    "      genre: z.string().describe(\"The genre of movies to search for\")\n",
    "    })\n",
    "  }\n",
    ");\n",
    "\n",
    "// More realistic tool that calls an API\n",
    "const getWeather = tool(\n",
    "  async ({ latitude, longitude }: { latitude: number; longitude: number }) => {\n",
    "    const url = \"https://api.open-meteo.com/v1/forecast\";\n",
    "    const params = new URLSearchParams({\n",
    "      latitude: latitude.toString(),\n",
    "      longitude: longitude.toString(),\n",
    "      current: \"temperature_2m,weather_code\",\n",
    "      temperature_unit: \"fahrenheit\"\n",
    "    });\n",
    "\n",
    "    const response = await fetch(`${url}?${params}`);\n",
    "    const data = await response.json() as any; // Type assertion for API response\n",
    "    const weather = data.current;\n",
    "    const temperature = weather.temperature_2m;\n",
    "    const weatherCode = weather.weather_code;\n",
    "    \n",
    "    return JSON.stringify({\n",
    "      temperature_fahrenheit: temperature,\n",
    "      weather_code: weatherCode\n",
    "    });\n",
    "  },\n",
    "  {\n",
    "    name: \"get_weather\",\n",
    "    description: \"Get current temperature in Fahrenheit and weather code for given coordinates. Returns JSON with temperature_fahrenheit and weather_code (do not include the code in your response, translate it to plain English)\",\n",
    "    schema: z.object({\n",
    "      latitude: z.number().describe(\"Latitude coordinate\"),\n",
    "      longitude: z.number().describe(\"Longitude coordinate\")\n",
    "    })\n",
    "  }\n",
    ");\n",
    "\n",
    "// Test a tool directly with SF's coordinates\n",
    "console.log(await getWeather.invoke({ latitude: 37.77, longitude: -122.42 }));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea91175",
   "metadata": {},
   "source": [
    "### Tool Calling (Function Calling)\n",
    "\n",
    "Now let's give these tools to the model using `.bindTools()`:\n",
    "\n",
    "> **Note:** For tool binding, we use `ChatOpenAI` directly instead of `initChatModel()` due to how the configurable model wrapper works in v1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a775f568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calls: [\n",
      "  {\n",
      "    name: 'get_weather',\n",
      "    args: { latitude: 47.6062, longitude: -122.3321 },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_CBKSPJfkqm2HsoHMLtm27Q2N'\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "// Bind tools to the model\n",
    "const tools = [getWeather, searchMovies];\n",
    "\n",
    "// For tool binding, we need to use ChatOpenAI directly (not initChatModel)\n",
    "const llm = await initChatModel(\"openai:gpt-4o-mini\")\n",
    "\n",
    "const modelWithTools = llm.bindTools(tools);\n",
    "\n",
    "const message = \"What's the weather like in Seattle?\";\n",
    "\n",
    "// The model can now decide to call tools\n",
    "const response4 = await modelWithTools.invoke(message);\n",
    "\n",
    "// Check if the model wants to call a tool\n",
    "console.log(\"Tool calls:\", response4.tool_calls);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9f4a35",
   "metadata": {},
   "source": [
    "The model returns a **tool call** request with:\n",
    "- `name`: Which tool to call\n",
    "- `args`: Arguments to pass to the tool\n",
    "- `id`: Unique identifier for tracking\n",
    "\n",
    "Let's execute the tool and continue the conversation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ed4e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Seattle is currently 51.8Â°F and clear skies.\n"
     ]
    }
   ],
   "source": [
    "import { ToolMessage } from \"langchain\";\n",
    "\n",
    "// Execute the tool call\n",
    "if (response4.tool_calls && response4.tool_calls.length > 0) {\n",
    "    const toolCall = response4.tool_calls[0];\n",
    "    \n",
    "    // Call the actual tool\n",
    "    let result;\n",
    "    if (toolCall.name === \"get_weather\") {\n",
    "        result = await getWeather.invoke(toolCall.args);\n",
    "    } else if (toolCall.name === \"search_movies\") {\n",
    "        result = await searchMovies.invoke(toolCall.args);\n",
    "    }\n",
    "    \n",
    "    // Create a ToolMessage with the result\n",
    "    const toolMessage = new ToolMessage({\n",
    "        content: result,\n",
    "        tool_call_id: toolCall.id\n",
    "    });\n",
    "    \n",
    "    // Continue the conversation with the tool result\n",
    "    const finalResponse = await modelWithTools.invoke([\n",
    "        new HumanMessage(message),\n",
    "        response4,\n",
    "        toolMessage\n",
    "    ]);\n",
    "    \n",
    "    console.log(finalResponse.content);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f373878d",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- Tools are functions wrapped with the `tool()` function\n",
    "- Good descriptions help the model know when to use each tool\n",
    "- Tool calling flow: Model requests tool â†’ Execute tool â†’ Return result â†’ Model synthesizes final response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac22108",
   "metadata": {},
   "source": [
    "## Part 4: Building Your First Agent with `createAgent()`\n",
    "\n",
    "Manually defining a specific sequence of LLM calls and tool calls is tedious and inflexible. Instead, we can use an **agent** that runs this loop:\n",
    "1. Model decides which tool to call (if any)\n",
    "2. Tool gets executed\n",
    "3. Result goes back to model\n",
    "4. Repeat until task is complete\n",
    "\n",
    "LangChain makes this easy with `createAgent()` - **build an agent in ~10 lines of code!**\n",
    "The prebuilt agent handles running the loop described above - you just specify the system prompt and tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c9de82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage]: What's the weather in NYC? Also recommend some sci-fi movies.\n",
      "[AIMessage]: \n",
      "[ToolMessage]: {\"temperature_fahrenheit\":50.2,\"weather_code\":3}\n",
      "[ToolMessage]: Dune, Interstellar, Blade Runner 2049\n",
      "[AIMessage]: The current temperature in New York City is 50.2Â°F, and the weather is partly cloudy.\n",
      "\n",
      "Here are some recommended sci-fi movies:\n",
      "1. **Dune**\n",
      "2. **Interstellar**\n",
      "3. **Blade Runner 2049**\n"
     ]
    }
   ],
   "source": [
    "import { createAgent } from \"langchain\";\n",
    "\n",
    "// Create an agent with tools\n",
    "const agent = createAgent({\n",
    "    model: \"openai:gpt-4o-mini\",\n",
    "    tools: [getWeather, searchMovies],\n",
    "    systemPrompt: \"You are a helpful assistant that can check weather and recommend movies.\"\n",
    "});\n",
    "\n",
    "// Use the agent\n",
    "const result = await agent.invoke({\n",
    "    messages: [{ role: \"user\", content: \"What's the weather in NYC? Also recommend some sci-fi movies.\" }]\n",
    "});\n",
    "\n",
    "// Print the conversation\n",
    "for (const msg of result.messages) {\n",
    "    console.log(`[${msg.constructor.name}]:`, msg.content);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ed05b",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "The agent automatically:\n",
    "1. Analyzed the user's request\n",
    "2. Called `get_weather` for NYC\n",
    "3. Called `search_movies` for \"sci-fi\"\n",
    "4. Synthesized the results into a natural response\n",
    "\n",
    "You can visualize the agent's structure (note: visualization requires additional setup):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93949b36",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- `createAgent()` builds a complete agent in ~10 lines\n",
    "- The agent automatically handles the reasoning â†’ action â†’ observation loop\n",
    "- Built on LangGraph for production features (persistence, streaming, human-in-the-loop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac60248",
   "metadata": {},
   "source": [
    "## Part 5: Adding Memory & State\n",
    "\n",
    "Right now, each agent invocation is independent. Let's add **memory** so the agent can maintain context across multiple interactions.\n",
    "\n",
    "LangGraph uses **checkpointers** to save and restore state:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc3b0a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: Hi Alice! Here are some sci-fi movies you might love:\n",
      "\n",
      "1. **Dune**\n",
      "2. **Interstellar**\n",
      "3. **Blade Runner 2049**\n",
      "\n",
      "Enjoy your movie watching!\n",
      "\n",
      "Response 2: Your name is Alice, and you love sci-fi movies.\n"
     ]
    }
   ],
   "source": [
    "import { MemorySaver } from \"@langchain/langgraph\";\n",
    "import { v4 as uuidv4 } from \"uuid\";\n",
    "\n",
    "// Create a checkpointer for memory\n",
    "const checkpointer = new MemorySaver();\n",
    "\n",
    "// Create an agent with memory\n",
    "const agentWithMemory = createAgent({\n",
    "    model: \"openai:gpt-4o-mini\",\n",
    "    tools: [getWeather, searchMovies],\n",
    "    systemPrompt: \"You are a helpful assistant.\",\n",
    "    checkpointer: checkpointer\n",
    "});\n",
    "\n",
    "// Create a thread for this conversation\n",
    "const threadId = uuidv4();\n",
    "const config = { configurable: { thread_id: threadId } };\n",
    "\n",
    "// First interaction\n",
    "const result1 = await agentWithMemory.invoke(\n",
    "    { messages: [{ role: \"user\", content: \"My name is Alice and I love sci-fi movies.\" }] },\n",
    "    config\n",
    ");\n",
    "\n",
    "console.log(\"Response 1:\", result1.messages[result1.messages.length - 1].content);\n",
    "\n",
    "// Second interaction - the agent remembers!\n",
    "const result2 = await agentWithMemory.invoke(\n",
    "    { messages: [{ role: \"user\", content: \"What's my name and what movies do I like?\" }] },\n",
    "    config\n",
    ");\n",
    "console.log(\"\\nResponse 2:\", result2.messages[result2.messages.length - 1].content);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d2c9c",
   "metadata": {},
   "source": [
    "### Understanding State & Threads\n",
    "\n",
    "- **State**: The agent's \"memory\" - includes message history and any custom data\n",
    "- **Thread**: A conversation session identified by `thread_id`\n",
    "- **Checkpointer**: Saves state after each step, enabling memory and error recovery\n",
    "\n",
    "Each thread is independent:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2469fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New thread response: I don't know your name. If you tell me your name, I can use it in our conversation!\n"
     ]
    }
   ],
   "source": [
    "// New thread - agent won't remember Alice\n",
    "const newThreadId = uuidv4();\n",
    "const newConfig = { configurable: { thread_id: newThreadId } };\n",
    "\n",
    "const result3 = await agentWithMemory.invoke(\n",
    "    { messages: [{ role: \"user\", content: \"What's my name?\" }] },\n",
    "    newConfig\n",
    ");\n",
    "console.log(\"New thread response:\", result3.messages[result3.messages.length - 1].content);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0622cc5",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- Checkpointers enable memory across interactions\n",
    "- Thread IDs separate different conversations\n",
    "- State persists automatically - no manual state management needed!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f519b4d5",
   "metadata": {},
   "source": [
    "## Part 6: Streaming for Better UX\n",
    "\n",
    "LLMs can take a while to respond. **Streaming** shows progress in real-time, dramatically improving user experience.\n",
    "\n",
    "LangChain supports multiple streaming modes:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ad592",
   "metadata": {},
   "source": [
    "### Streaming Agent Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aead9eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming agent steps:\n",
      "\n",
      "Step: model_request\n",
      "   Tool call: get_weather\n",
      "\n",
      "Step: tools\n",
      "   Content: {\"temperature_fahrenheit\":47.7,\"weather_code\":3}\n",
      "\n",
      "Step: model_request\n",
      "   Content: The current temperature in Boston is 47.7Â°F, and the weather is partly cloudy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Stream agent progress with streamMode=\"updates\"\n",
    "console.log(\"Streaming agent steps:\\n\");\n",
    "\n",
    "for await (const chunk of await agent.stream(\n",
    "    { messages: [{ role: \"user\", content: \"What's the weather in Boston?\" }] },\n",
    "    { streamMode: \"updates\" } as any\n",
    ")) {\n",
    "    for (const [nodeName, data] of Object.entries(chunk)) {\n",
    "        console.log(`Step: ${nodeName}`);\n",
    "        // Type guard to check if data has messages property\n",
    "        if (data && typeof data === \"object\" && \"messages\" in data) {\n",
    "            const messages = data.messages as any[];\n",
    "            const message = messages[messages.length - 1];\n",
    "            if (message.tool_calls && message.tool_calls.length > 0) {\n",
    "                console.log(`   Tool call: ${message.tool_calls[0].name}`);\n",
    "            } else if (message.content) {\n",
    "                const content = message.content.length > 100 \n",
    "                    ? `${message.content.substring(0, 100)}...` \n",
    "                    : message.content;\n",
    "                console.log(`   Content: ${content}`);\n",
    "            }\n",
    "        }\n",
    "        console.log();\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f1fc3",
   "metadata": {},
   "source": [
    "### Streaming LLM Tokens\n",
    "\n",
    "For a ChatGPT-like experience, stream tokens as they're generated:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92c04394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming tokens:\n",
      "\n",
      "LangGraph is a platform that leverages language models to analyze and visualize data relationships through graph-based representations, facilitating better understanding and insights from complex information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Stream tokens - simplified ChatGPT-like experience\n",
    "console.log(\"Streaming tokens:\\n\");\n",
    "\n",
    "for await (const [token, metadata] of await agent.stream(\n",
    "    { messages: [{ role: \"user\", content: \"Tell me about LangGraph in one sentence.\" }] },\n",
    "    { streamMode: \"messages\" } as any\n",
    ")) {\n",
    "    // Print any text content we find\n",
    "    if (token.text) {\n",
    "        process.stdout.write(token.text);\n",
    "    }\n",
    "}\n",
    "\n",
    "console.log(\"\\n\");  // New line at the end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543acd63",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- `streamMode: \"updates\"` - See each agent step (useful for debugging)\n",
    "- `streamMode: \"messages\"` - Stream LLM tokens (ChatGPT-like UX)\n",
    "- Streaming is built-in - no extra setup required!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99e4c1",
   "metadata": {},
   "source": [
    "## Part 7: Putting It All Together - A Practical Example\n",
    "\n",
    "Let's build a more realistic agent that combines everything we've learned:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc5e9aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PERSONAL ASSISTANT DEMO\n",
      "==================================================\n",
      "\n",
      "User: Hi, I'm Alice. Can you check my preferences and recommend a movie?\n",
      "\n",
      "Assistant: Hi Alice! I've checked your preferences, and it's great to know that you love sci-fi movies. Here are some recommendations for you:\n",
      "\n",
      "1. **Arrival** - A thought-provoking take on communication and time.\n",
      "2. **Ex Machina** - A thrilling exploration of artificial intelligence.\n",
      "3. **The Martian** - An inspiring survival story with a touch of humor.\n",
      "\n",
      "Enjoy your movie time! Let me know if you need anything else.\n",
      "\n",
      "User: Also, what's the weather like in San Francisco?\n",
      "\n",
      "Assistant: The weather in San Francisco is quite pleasant right now, with a temperature of 66Â°F and clear skies. Perfect for a stroll or some outdoor activities! Let me know if there's anything else you'd like to know.\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "// Create more realistic tools\n",
    "const getUserPreferences = tool(\n",
    "  async ({ userId }: { userId: string }) => {\n",
    "    // Simulate a user database\n",
    "    const preferences: Record<string, string> = {\n",
    "      \"alice\": \"Loves sci-fi movies, prefers warm weather destinations\",\n",
    "      \"bob\": \"Enjoys comedy films, likes cold climates for travel\"\n",
    "    };\n",
    "    return preferences[userId.toLowerCase()] || \"No preferences found\";\n",
    "  },\n",
    "  {\n",
    "    name: \"get_user_preferences\",\n",
    "    description: \"Get a user's saved preferences.\",\n",
    "    schema: z.object({\n",
    "      userId: z.string().describe(\"The user ID to look up\")\n",
    "    })\n",
    "  }\n",
    ");\n",
    "\n",
    "const bookRecommendation = tool(\n",
    "  async ({ genre, userPreferences }: { genre: string; userPreferences?: string }) => {\n",
    "    const recommendations: Record<string, string> = {\n",
    "      \"sci-fi\": \"Based on your preferences, try: Arrival, Ex Machina, or The Martian\",\n",
    "      \"comedy\": \"Based on your preferences, try: The Big Lebowski, Anchorman, or Bridesmaids\"\n",
    "    };\n",
    "    return recommendations[genre.toLowerCase()] || \"No recommendations available\";\n",
    "  },\n",
    "  {\n",
    "    name: \"book_recommendation\",\n",
    "    description: \"Get personalized movie recommendations based on genre and user preferences.\",\n",
    "    schema: z.object({\n",
    "      genre: z.string().describe(\"The movie genre\"),\n",
    "      userPreferences: z.string().optional().describe(\"Optional user preferences\")\n",
    "    })\n",
    "  }\n",
    ");\n",
    "\n",
    "// Create a helpful assistant agent\n",
    "const assistant = createAgent({\n",
    "    model: \"openai:gpt-4o-mini\",\n",
    "    tools: [getWeather, getUserPreferences, bookRecommendation],\n",
    "    systemPrompt: `You are a helpful personal assistant. \n",
    "    \n",
    "    You can:\n",
    "    - Check weather for any city\n",
    "    - Look up user preferences\n",
    "    - Recommend movies based on preferences\n",
    "    \n",
    "    Always be friendly and personalize your responses based on user preferences.`,\n",
    "    checkpointer: new MemorySaver()\n",
    "});\n",
    "\n",
    "// Demo conversation\n",
    "const demoThreadId = uuidv4();\n",
    "const demoConfig = { configurable: { thread_id: demoThreadId } };\n",
    "\n",
    "console.log(\"=\".repeat(50));\n",
    "console.log(\"PERSONAL ASSISTANT DEMO\");\n",
    "console.log(\"=\".repeat(50) + \"\\n\");\n",
    "\n",
    "// Interaction 1\n",
    "console.log(\"User: Hi, I'm Alice. Can you check my preferences and recommend a movie?\\n\");\n",
    "const demoResult1 = await assistant.invoke(\n",
    "    { messages: [{ role: \"user\", content: \"Hi, I'm Alice. Can you check my preferences and recommend a movie?\" }] },\n",
    "    demoConfig\n",
    ");\n",
    "console.log(`Assistant: ${demoResult1.messages[demoResult1.messages.length - 1].content}\\n`);\n",
    "\n",
    "// Interaction 2\n",
    "console.log(\"User: Also, what's the weather like in San Francisco?\\n\");\n",
    "const demoResult2 = await assistant.invoke(\n",
    "    { messages: [{ role: \"user\", content: \"Also, what's the weather like in San Francisco?\" }] },\n",
    "    demoConfig\n",
    ");\n",
    "console.log(`Assistant: ${demoResult2.messages[demoResult2.messages.length - 1].content}\\n`);\n",
    "\n",
    "console.log(\"=\".repeat(50));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e4c440",
   "metadata": {},
   "source": [
    "## Part 8: Next Steps - Exploring LangGraph Primitives\n",
    "\n",
    "We've been using `createAgent()`, which is built on **LangGraph**. LangGraph gives you full control over agent behavior using three core primitives:\n",
    "\n",
    "### Core LangGraph Concepts:\n",
    "\n",
    "1. **State**\n",
    "   - Shared data structure passed between nodes\n",
    "   - Represents the agent's \"memory\"\n",
    "   - Can include messages, custom data, etc.\n",
    "\n",
    "2. **Nodes**\n",
    "   - Functions that process state\n",
    "   - Each node performs a specific task\n",
    "   - Examples: call LLM, execute tool, validate input\n",
    "\n",
    "3. **Edges**\n",
    "   - Define flow between nodes\n",
    "   - Can be normal (always go to next node)\n",
    "   - Or conditional (decide based on logic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b5a9a",
   "metadata": {},
   "source": [
    "### When to use `createAgent()` vs custom LangGraph?\n",
    "\n",
    "**Use `createAgent()` when:**\n",
    "- Building standard ReAct-style agents\n",
    "- You need quick prototyping\n",
    "- Default behavior works for your use case\n",
    "\n",
    "**Use custom LangGraph when:**\n",
    "- You need custom control flow (e.g., approval workflows)\n",
    "- Building multi-agent systems\n",
    "- Implementing human-in-the-loop patterns\n",
    "- Complex state management requirements\n",
    "\n",
    "For more advanced patterns, check out:\n",
    "- [LangGraph Documentation](https://docs.langchain.com/oss/javascript/langgraph/overview)\n",
    "- [LangChain Academy](https://academy.langchain.com/)\n",
    "- The `multi_agent.ipynb` notebook in this repo (LangGraph 201)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6092e9dc",
   "metadata": {},
   "source": [
    "## Building a ReAct Agent from Scratch\n",
    "\n",
    "To explore how LangGraph's primitives work, let's rebuild the agent we created above, but without `createAgent()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e00759a",
   "metadata": {},
   "source": [
    "### State\n",
    "\n",
    "Our chatbot is simple - just an LLM with some associated tools - so all we need to keep in our state is a list of human messages, AI messages, and tool messages that grows as the conversation goes on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3cde473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State defined!\n"
     ]
    }
   ],
   "source": [
    "import { Annotation, messagesStateReducer } from \"@langchain/langgraph\";\n",
    "import { BaseMessage } from \"langchain\";\n",
    "\n",
    "// Define our State\n",
    "var StateAnnotation = Annotation.Root({\n",
    "  messages: Annotation<BaseMessage[]>({\n",
    "    reducer: messagesStateReducer,\n",
    "  }),\n",
    "});\n",
    "\n",
    "console.log(\"State defined!\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed81bee",
   "metadata": {},
   "source": [
    "### Nodes\n",
    "\n",
    "We can reuse the tools we defined above. However, we need to create a node that handles tool calls and formats the results as `ToolMessage` objects for the LLM to work with. Instead of defining the node ourselves, we'll use LangGraph's prebuilt `ToolNode`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed5c659f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool node created!\n"
     ]
    }
   ],
   "source": [
    "// Workaround for tslab module resolution\n",
    "var { ToolNode } = require(\"@langchain/langgraph/prebuilt\");\n",
    "\n",
    "// Create tool node with our existing tools\n",
    "var reactTools = [searchMovies, getWeather];\n",
    "var toolNode = new ToolNode(reactTools);\n",
    "\n",
    "console.log(\"Tool node created!\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3bcd4c",
   "metadata": {},
   "source": [
    "Now, let's create our node for the main assistant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc5f1be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant node defined!\n"
     ]
    }
   ],
   "source": [
    "// Assistant node that calls the LLM\n",
    "async function assistant(state: typeof StateAnnotation.State) {\n",
    "  // Create a list of messages to send to the LLM, beginning with our fixed system prompt\n",
    "  const systemPrompt = \"You are a helpful assistant that can check weather and recommend movies.\";\n",
    "  const allMessages = [new SystemMessage(systemPrompt), ...state.messages];\n",
    "  \n",
    "  // Invoke the LLM with tools bound\n",
    "  const response = await modelWithTools.invoke(allMessages);\n",
    "  \n",
    "  // Update the state with the response from the LLM\n",
    "  return { messages: [response] };\n",
    "}\n",
    "\n",
    "console.log(\"Assistant node defined!\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8417a",
   "metadata": {},
   "source": [
    "Now, we need to define a control flow that connects between our defined nodes, and that's where the concept of edges come in.\n",
    "\n",
    "**Edges are connections between nodes. They define the flow of the graph.**\n",
    "* **Normal edges** are deterministic and always go from one node to its defined target\n",
    "* **Conditional edges** are used to dynamically route between nodes, implemented as functions that return the next node to visit based upon some logic.\n",
    "\n",
    "In this case, we want a **conditional edge** from our assistant that determines whether to:\n",
    "- Invoke tools, or,\n",
    "- Route to the end if user query has been finished\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9553c57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional edge defined!\n"
     ]
    }
   ],
   "source": [
    "import { AIMessage } from \"langchain\";\n",
    "\n",
    "// Conditional edge that determines whether to continue or not\n",
    "function shouldContinue(state: typeof StateAnnotation.State): \"continue\" | \"end\" {\n",
    "  const messages = state.messages;\n",
    "  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
    "  \n",
    "  // The LLM wants to make tool calls, we should execute them and continue\n",
    "  if (lastMessage.tool_calls && lastMessage.tool_calls.length > 0) {\n",
    "    return \"continue\";\n",
    "  }\n",
    "  // The LLM returned a response instead of tool calls, we're finished\n",
    "  else {\n",
    "    return \"end\";\n",
    "  }\n",
    "}\n",
    "\n",
    "console.log(\"Conditional edge defined!\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31441a38",
   "metadata": {},
   "source": [
    "#### Compile Graph!\n",
    "\n",
    "Now that we've defined our state, nodes, and conditional edge let's put it all together and construct our react agent!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "511262c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReAct agent compiled!\n"
     ]
    }
   ],
   "source": [
    "import { StateGraph, START, END } from \"@langchain/langgraph\";\n",
    "\n",
    "// Start by defining a builder with our State class\n",
    "var builder = new StateGraph(StateAnnotation)\n",
    "  // Add nodes\n",
    "  .addNode(\"assistant\", assistant)\n",
    "  .addNode(\"tool_node\", toolNode)\n",
    "  // Add edges\n",
    "  // First, we define the start node. The query will always route to the assistant node first.\n",
    "  .addEdge(START, \"assistant\")\n",
    "  // Add in the conditional edge after we call the LLM\n",
    "  .addConditionalEdges(\"assistant\", shouldContinue, {\n",
    "    // If `continue`, then we call the tool node.\n",
    "    continue: \"tool_node\",\n",
    "    // Otherwise we finish.\n",
    "    end: END,\n",
    "  })\n",
    "  // Always return to the LLM after calling tools\n",
    "  .addEdge(\"tool_node\", \"assistant\");\n",
    "\n",
    "// Compile the graph\n",
    "var reactAgent = builder.compile();\n",
    "\n",
    "console.log(\"ReAct agent compiled!\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f40846",
   "metadata": {},
   "source": [
    "We have successfully rebuilt a ReAct agent from scratch identical to the basic one we made using `createAgent`. Let's try it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6774f686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage]: What is the weather in SF today, and what are some good Sci-Fi movies?\n",
      "[AIMessage]: \n",
      "[ToolMessage]: {\"temperature_fahrenheit\":66,\"weather_code\":0}\n",
      "[ToolMessage]: Dune, Interstellar, Blade Runner 2049\n",
      "[AIMessage]: Today in San Francisco, the temperature is 66Â°F and the weather is clear. \n",
      "\n",
      "As for Sci-Fi movies, here are some great options you might enjoy:\n",
      "1. **Dune**\n",
      "2. **Interstellar**\n",
      "3. **Blade Runner 2049** \n",
      "\n",
      "Enjoy the weather and the movies!\n"
     ]
    }
   ],
   "source": [
    "// Test the custom-built ReAct agent\n",
    "const question = \"What is the weather in SF today, and what are some good Sci-Fi movies?\";\n",
    "\n",
    "const result = await reactAgent.invoke({ messages: [new HumanMessage(question)] });\n",
    "\n",
    "// Print all messages\n",
    "for (const message of result.messages) {\n",
    "  console.log(`[${message.constructor.name}]:`, message.content);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4bfcb9",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've learned the core concepts of building agents with LangChain and LangGraph:\n",
    "\n",
    "âœ… **Models** - Standardized interface across providers  \n",
    "âœ… **Messages** - Building block of conversations  \n",
    "âœ… **Tools** - Extending LLM capabilities  \n",
    "âœ… **Agents** - Automated reasoning and action loops  \n",
    "âœ… **Memory** - Maintaining context across interactions  \n",
    "âœ… **Streaming** - Real-time user experience  \n",
    "âœ… **LangGraph** - The foundation powering it all\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "1. **Build your own agent** with your specific tools and use case\n",
    "2. **Explore advanced patterns** in the `multi_agent.ipynb` notebook\n",
    "3. **Add debugging** with [LangSmith](https://smith.langchain.com)\n",
    "4. **Deploy to production** using LangGraph's persistence and error recovery\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [LangChain Documentation](https://docs.langchain.com/oss/javascript/langchain/overview)\n",
    "- [LangGraph Documentation](https://docs.langchain.com/oss/javascript/langgraph/overview)\n",
    "- [LangSmith for Debugging](https://smith.langchain.com)\n",
    "- [LangChain Academy](https://academy.langchain.com/)\n",
    "<br>\n",
    "<br>\n",
    "---\n",
    "<br>\n",
    "\n",
    "**Happy building!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
