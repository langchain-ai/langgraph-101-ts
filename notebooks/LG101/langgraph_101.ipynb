{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6e5cee",
   "metadata": {},
   "source": [
    "# LangGraph 101: Building Your First Agent (TypeScript)\n",
    "\n",
    "Welcome to LangGraph 101! This notebook will walk you through the core concepts of building agents with LangChain and LangGraph using TypeScript.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to interact with language models\n",
    "- Working with messages and conversation\n",
    "- Adding tools to extend LLM capabilities\n",
    "- Building an agent that can reason and act\n",
    "- Adding memory to maintain context\n",
    "- Streaming responses for better UX\n",
    "<br>\n",
    "<br>\n",
    "---\n",
    "<br>\n",
    "\n",
    "> **Note:** This tutorial uses LangChain v1, which provides the easiest way to start building with LLMs. LangChain agents are built on top of LangGraph, providing durable execution, streaming, human-in-the-loop, and persistence out of the box.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c09d828",
   "metadata": {},
   "source": [
    "## üöÄ Quick Start Checklist\n",
    "\n",
    "Before running this notebook, make sure you've completed these steps:\n",
    "\n",
    "1. ‚úÖ Installed Node.js (v20+) and pnpm\n",
    "2. ‚úÖ Run `pnpm install` from `/Users/victormoreira/Desktop/demos/langgraph-101-ts`\n",
    "3. ‚úÖ Created a `.env` file with your `OPENAI_API_KEY`\n",
    "4. ‚úÖ Installed tslab: `npm install -g tslab && tslab install`\n",
    "5. ‚úÖ Selected \"TypeScript\" as your kernel (top-right in VS Code/Jupyter)\n",
    "\n",
    "**Tips:**\n",
    "- Wait 2-3 seconds between running cells\n",
    "- If you get errors, restart the kernel and try again\n",
    "- Full setup guide is in the [README.md](../../README.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d8668a",
   "metadata": {},
   "source": [
    "## Part 0: Setup & Installation\n",
    "\n",
    "First, let's install the necessary packages and set up our environment.\n",
    "\n",
    "> **‚ö†Ô∏è Important Notes:**\n",
    "> - **Before running this notebook**, make sure you've run `pnpm install` from the project root\n",
    "> - **Wait a few seconds** between running cells to avoid tslab timing issues\n",
    "> - If you get a \"rebuildTimer\" error, **restart the kernel** and try again\n",
    "> - See the README.md for full troubleshooting guide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Make sure you've run 'pnpm install' from the project root before continuing!\n"
     ]
    }
   ],
   "source": [
    "// IMPORTANT: Before running this notebook, install dependencies from the project root:\n",
    "// \n",
    "// cd /Users/victormoreira/Desktop/demos/langgraph-101-ts\n",
    "// pnpm install\n",
    "// p\n",
    "// This will install: langchain, @langchain/core, @langchain/langgraph, \n",
    "// @langchain/openai, zod, uuid, and dotenv\n",
    "\n",
    "console.log(\"‚úì Make sure you've run 'pnpm install' from the project root before continuing!\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10fa1d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Environment loaded successfully!\n",
      "\n",
      "üìù Make sure OPENAI_API_KEY is set in your .env file or environment\n"
     ]
    }
   ],
   "source": [
    "// Load environment variables\n",
    "try {\n",
    "    await import(\"dotenv/config\");\n",
    "    console.log(\"‚úì Environment loaded successfully!\");\n",
    "} catch (error) {\n",
    "    console.log(\"‚ö†Ô∏è  Could not load dotenv. Make sure you've run 'pnpm install' from the project root.\");\n",
    "    console.log(\"‚ö†Ô∏è  You can continue if you've set OPENAI_API_KEY as a system environment variable.\");\n",
    "}\n",
    "\n",
    "// We'll use OpenAI in this tutorial, but you can swap to any provider!\n",
    "// Supported providers: OpenAI, Anthropic, Google, and many more\n",
    "console.log(\"\\nüìù Make sure OPENAI_API_KEY is set in your .env file or environment\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d6f6c",
   "metadata": {},
   "source": [
    "## Part 1: Your First LLM Call\n",
    "\n",
    "LangChain provides a **standard model interface** that works across all providers. This means you can easily swap between OpenAI, Anthropic, Google, and other providers without changing your code.\n",
    "\n",
    "Let's start by initializing a chat model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74208dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is an open-source framework for building applications that use large language models (LLMs). It provides reusable building blocks to connect LLMs to data sources, tools, memory, and external services, so you can create chatbots, question-answering systems, automation agents, and other LLM-powered apps more easily.\n",
      "\n",
      "Key concepts it offers:\n",
      "- LLM: Wrappers around language models (e.g., OpenAI, Cohere, local models).\n",
      "- Prompts and Prompt templates: Structured ways to compose prompts.\n",
      "- Chains: Sequences of steps (prompt + model calls + post-processing) to transform inputs into outputs.\n",
      "- Tools: External APIs or functions that an app can call (search, compute, fetch data, etc.).\n",
      "- Agents: Decision-makers that choose which tools to use and when, based on the LLM‚Äôs output.\n",
      "- Memory: Keeps context across interactions (useful for chatbots and ongoing conversations).\n",
      "- Vector stores and document loaders: For retrieval-augmented generation (RAG) and knowledge-grounded QA.\n",
      "\n",
      "Common uses:\n",
      "- Chatbots and virtual assistants with memory\n",
      "- Retrieval-augmented generation (RAG) over documents or knowledge bases\n",
      "- Automated workflows that call external tools or APIs\n",
      "- Data analysis and summarization pipelines\n",
      "\n",
      "Languages and setup:\n",
      "- Primary support is for Python (LangChain) and JavaScript/TypeScript (LangChain.js)\n",
      "- Installation example: pip install langchain (plus any provider packages like openai)\n",
      "\n",
      "Simple usage idea:\n",
      "- Define an LLM (e.g., OpenAI), a prompt template, and a chain, then run the chain with your input to get a structured result.\n",
      "- Or build an Agent with Tools to let the app decide when to call APIs and how to respond.\n",
      "\n",
      "If you‚Äôd like, I can give you a quick code example in Python to illustrate a basic chain or a simple agent, or point you to a starter project.\n"
     ]
    }
   ],
   "source": [
    "import { initChatModel } from \"langchain\";\n",
    "\n",
    "// Initialize a chat model - you can easily swap providers!\n",
    "// Examples: \"openai:gpt-4o\", \"anthropic:claude-3-7-sonnet-latest\", \"google:gemini-2.0-flash\"\n",
    "const model = await initChatModel(\"openai:gpt-5-nano\");\n",
    "\n",
    "// Make your first call!\n",
    "const response = await model.invoke(\"What is LangChain?\");\n",
    "console.log(response.content);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf76ee",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- `initChatModel()` gives you a standardized interface to any LLM provider\n",
    "- `.invoke()` sends a message and returns a response\n",
    "- No provider lock-in - swap models easily!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7850a575",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Messages\n",
    "\n",
    "**Messages** are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both content and metadata.\n",
    "\n",
    "There are different message types:\n",
    "- **SystemMessage** - Instructions for how the model should behave\n",
    "- **HumanMessage** - User input\n",
    "- **AIMessage** - Model responses\n",
    "- **ToolMessage** - Results from tool executions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b48d1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An agent is any entity that can sense its surroundings and take actions to achieve a goal.\n",
      "\n",
      "Key ideas:\n",
      "- Senses: It uses sensors to perceive the environment.\n",
      "- Acts: It uses actuators or some mechanism to affect the environment.\n",
      "- Goal-directed: It has objectives or goals it tries to achieve.\n",
      "- Autonomy: It operates on its own, making decisions based on what it senses and what it knows.\n",
      "\n",
      "Common types and examples:\n",
      "- Simple reflex agent: Responds to current inputs with fixed actions (e.g., a light switch that turns on when it‚Äôs dark).\n",
      "- Thermostat: Perceives temperature and turns the heater on/off to reach a target temperature.\n",
      "- Self-driving car: Senses road conditions, traffic, and pedestrians, and makes driving decisions.\n",
      "- Software agent (e.g., an email spam filter): Perceives incoming mail and decides whether to mark it as spam.\n",
      "\n",
      "Components you‚Äôll hear about:\n",
      "- Sensors: How the agent perceives the world.\n",
      "- Decision maker: How it chooses actions (rules, models, learning).\n",
      "- Actuators: How it changes the world (robot wheels, software actions, etc.).\n",
      "- Knowledge/model: Information about the world it uses to decide.\n",
      "- Sometimes learning: It can improve its decisions over time.\n",
      "\n",
      "Important notes:\n",
      "- An agent is not necessarily human or intelligent; it can be a simple rule-based system.\n",
      "- In AI, ‚Äúagent‚Äù often implies some level of autonomy and goal-directed behavior, possibly in a complex, changing environment.\n",
      "- In multi-agent systems, multiple agents interact and may cooperate or compete.\n",
      "\n",
      "If you have a particular domain in mind (robots, software, business processes), I can tailor the explanation with domain-specific examples.\n"
     ]
    }
   ],
   "source": [
    "import { HumanMessage, SystemMessage } from \"langchain\";\n",
    "\n",
    "// Create a conversation with different message types\n",
    "const messages = [\n",
    "    new SystemMessage(\"You are a helpful AI assistant that explains technical concepts simply.\"),\n",
    "    new HumanMessage(\"What is an agent?\"),\n",
    "];\n",
    "\n",
    "const response2 = await model.invoke(messages);\n",
    "console.log(response2.content);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2115b2d",
   "metadata": {},
   "source": [
    "### Multi-turn Conversations\n",
    "\n",
    "Messages make it easy to maintain conversation history:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b40774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure. Here‚Äôs a concrete, simple example: a smart home thermostat (the heating system in a room).\n",
      "\n",
      "- Goal: Keep the room at a comfortable temperature while saving energy.\n",
      "- Environment: A room with a heater, a temperature sensor, and maybe an occupancy sensor.\n",
      "- Sensors: Temperature sensor reports current_temp (e.g., 20¬∞C).\n",
      "- Decision maker: A small set of rules (rule-based). For example, target_temp is 22¬∞C and a margin of 0.5¬∞C.\n",
      "  - If current_temp < target_temp ‚àí 0.5, turn the heater ON.\n",
      "  - If current_temp > target_temp + 0.5, turn the heater OFF.\n",
      "  - Otherwise, do nothing.\n",
      "- Actions (Actuators): The heater (and possibly a fan) are turned ON or OFF, or the heater‚Äôs power level is adjusted.\n",
      "- Knowledge/Model: The desired temperature (target_temp) and the tolerance margin; perhaps a simple policy about when to run the heater.\n",
      "- Learning (optional): It could learn daily patterns to adjust target_temp for different times of day.\n",
      "\n",
      "Example run:\n",
      "- Outside is cold; current_temp is 20¬∞C, target is 22¬∞C, margin is 0.5¬∞C.\n",
      "- Since 20 < 22 ‚àí 0.5, the thermostat turns the heater ON.\n",
      "- As it warms and current_temp rises to around 22¬∞C, once it exceeds 22.5¬∞C, the heater turns OFF.\n",
      "- The cycle repeats as needed to maintain comfort.\n",
      "\n",
      "This illustrates an agent: it senses, has a goal, uses a simple decision rule, and acts to change the environment. If you‚Äôd like a different domain (e.g., a robot vacuum or a chat assistant), I can give another tailored example.\n"
     ]
    }
   ],
   "source": [
    "// Continue the conversation\n",
    "messages.push(response2);  // Add AI response to history\n",
    "messages.push(new HumanMessage(\"Can you give me an example?\"));\n",
    "\n",
    "const response3 = await model.invoke(messages);\n",
    "console.log(response3.content);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc5d969",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- Messages represent the conversation history\n",
    "- SystemMessage sets the model's behavior\n",
    "- Build multi-turn conversations by appending messages to an array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d7140",
   "metadata": {},
   "source": [
    "## Part 3: Adding Tools - Extending LLM Capabilities\n",
    "\n",
    "LLMs are great at language, but they can't access external data or perform actions. **Tools** extend their capabilities. You can give an LLM a list of tools, and when it needs one, it will specify which tool to call. Your job is to execute the tool and feed the results back to the LLM so it can decide what to do next.\n",
    "\n",
    "You can create a tool just by writing a function with a clear description. LangChain's `tool` function handles formatting the function's information in the LLM's desired format.\n",
    "\n",
    "Let's create some simple tools:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca7e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"temperature_fahrenheit\":63.8,\"weather_code\":0}\n"
     ]
    }
   ],
   "source": [
    "import { tool } from \"langchain\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "// Basic hardcoded tool\n",
    "const searchMovies = tool(\n",
    "  async ({ genre }: { genre: string }) => {\n",
    "    // In a real app, this would query a movie database\n",
    "    const movies: Record<string, string> = {\n",
    "      \"sci-fi\": \"Dune, Interstellar, Blade Runner 2049\",\n",
    "      \"comedy\": \"The Grand Budapest Hotel, Superbad, Knives Out\",\n",
    "      \"action\": \"Mad Max: Fury Road, John Wick, Mission Impossible\"\n",
    "    };\n",
    "    return movies[genre.toLowerCase()] || \"No movies found for that genre\";\n",
    "  },\n",
    "  {\n",
    "    name: \"search_movies\",\n",
    "    description: \"Search for movies by genre.\",\n",
    "    schema: z.object({\n",
    "      genre: z.string().describe(\"The genre of movies to search for\")\n",
    "    })\n",
    "  }\n",
    ");\n",
    "\n",
    "// More realistic tool that calls an API\n",
    "const getWeather = tool(\n",
    "  async ({ latitude, longitude }: { latitude: number; longitude: number }) => {\n",
    "    const url = \"https://api.open-meteo.com/v1/forecast\";\n",
    "    const params = new URLSearchParams({\n",
    "      latitude: latitude.toString(),\n",
    "      longitude: longitude.toString(),\n",
    "      current: \"temperature_2m,weather_code\",\n",
    "      temperature_unit: \"fahrenheit\"\n",
    "    });\n",
    "\n",
    "    const response = await fetch(`${url}?${params}`);\n",
    "    const data = await response.json() as any; // Type assertion for API response\n",
    "    const weather = data.current;\n",
    "    const temperature = weather.temperature_2m;\n",
    "    const weatherCode = weather.weather_code;\n",
    "    \n",
    "    return JSON.stringify({\n",
    "      temperature_fahrenheit: temperature,\n",
    "      weather_code: weatherCode\n",
    "    });\n",
    "  },\n",
    "  {\n",
    "    name: \"get_weather\",\n",
    "    description: \"Get current temperature in Fahrenheit and weather code for given coordinates. Returns JSON with temperature_fahrenheit and weather_code (do not include the code in your response, translate it to plain English)\",\n",
    "    schema: z.object({\n",
    "      latitude: z.number().describe(\"Latitude coordinate\"),\n",
    "      longitude: z.number().describe(\"Longitude coordinate\")\n",
    "    })\n",
    "  }\n",
    ");\n",
    "\n",
    "// Test a tool directly with SF's coordinates\n",
    "console.log(await getWeather.invoke({ latitude: 37.77, longitude: -122.42 }));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea91175",
   "metadata": {},
   "source": [
    "### Tool Calling (Function Calling)\n",
    "\n",
    "Now let's give these tools to the model using `.bindTools()`:\n",
    "\n",
    "> **Note:** For tool binding, we use `ChatOpenAI` directly instead of `initChatModel()` due to how the configurable model wrapper works in v1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a775f568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calls: [\n",
      "  {\n",
      "    name: 'get_weather',\n",
      "    args: { latitude: 47.6062, longitude: -122.3321 },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_rxHBfz5QLEyp34kdbOd1ndP0'\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "// Bind tools to the model\n",
    "const tools = [getWeather, searchMovies];\n",
    "\n",
    "// For tool binding, we need to use ChatOpenAI directly (not initChatModel)\n",
    "const llm = await initChatModel(\"openai:gpt-4o-mini\")\n",
    "\n",
    "const modelWithTools = llm.bindTools(tools);\n",
    "\n",
    "const message = \"What's the weather like in Seattle?\";\n",
    "\n",
    "// The model can now decide to call tools\n",
    "const response4 = await modelWithTools.invoke(message);\n",
    "\n",
    "// Check if the model wants to call a tool\n",
    "console.log(\"Tool calls:\", response4.tool_calls);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9f4a35",
   "metadata": {},
   "source": [
    "The model returns a **tool call** request with:\n",
    "- `name`: Which tool to call\n",
    "- `args`: Arguments to pass to the tool\n",
    "- `id`: Unique identifier for tracking\n",
    "\n",
    "Let's execute the tool and continue the conversation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ed4e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Seattle is currently 49.6¬∞F, and it's drizzling light rain.\n"
     ]
    }
   ],
   "source": [
    "import { ToolMessage } from \"langchain\";\n",
    "\n",
    "// Execute the tool call\n",
    "if (response4.tool_calls && response4.tool_calls.length > 0) {\n",
    "    const toolCall = response4.tool_calls[0];\n",
    "    \n",
    "    // Call the actual tool\n",
    "    let result;\n",
    "    if (toolCall.name === \"get_weather\") {\n",
    "        result = await getWeather.invoke(toolCall.args);\n",
    "    } else if (toolCall.name === \"search_movies\") {\n",
    "        result = await searchMovies.invoke(toolCall.args);\n",
    "    }\n",
    "    \n",
    "    // Create a ToolMessage with the result\n",
    "    const toolMessage = new ToolMessage({\n",
    "        content: result,\n",
    "        tool_call_id: toolCall.id\n",
    "    });\n",
    "    \n",
    "    // Continue the conversation with the tool result\n",
    "    const finalResponse = await modelWithTools.invoke([\n",
    "        new HumanMessage(message),\n",
    "        response4,\n",
    "        toolMessage\n",
    "    ]);\n",
    "    \n",
    "    console.log(finalResponse.content);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f373878d",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- Tools are functions wrapped with the `tool()` function\n",
    "- Good descriptions help the model know when to use each tool\n",
    "- Tool calling flow: Model requests tool ‚Üí Execute tool ‚Üí Return result ‚Üí Model synthesizes final response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac22108",
   "metadata": {},
   "source": [
    "## Part 4: Building Your First Agent with `createAgent()`\n",
    "\n",
    "Manually defining a specific sequence of LLM calls and tool calls is tedious and inflexible. Instead, we can use an **agent** that runs this loop:\n",
    "1. Model decides which tool to call (if any)\n",
    "2. Tool gets executed\n",
    "3. Result goes back to model\n",
    "4. Repeat until task is complete\n",
    "\n",
    "LangChain makes this easy with `createAgent()` - **build an agent in ~10 lines of code!**\n",
    "The prebuilt agent handles running the loop described above - you just specify the system prompt and tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c9de82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage]: What's the weather in NYC? Also recommend some sci-fi movies.\n",
      "[AIMessage]: \n",
      "[ToolMessage]: {\"temperature_fahrenheit\":48.3,\"weather_code\":3}\n",
      "[ToolMessage]: Dune, Interstellar, Blade Runner 2049\n",
      "[AIMessage]: The current weather in New York City is 48.3¬∞F, and it's mostly cloudy.\n",
      "\n",
      "Here are some sci-fi movie recommendations for you:\n",
      "1. **Dune**\n",
      "2. **Interstellar**\n",
      "3. **Blade Runner 2049** \n",
      "\n",
      "Enjoy your day!\n"
     ]
    }
   ],
   "source": [
    "import { createAgent } from \"langchain\";\n",
    "\n",
    "// Create an agent with tools\n",
    "const agent = createAgent({\n",
    "    model: \"openai:gpt-4o-mini\",\n",
    "    tools: [getWeather, searchMovies],\n",
    "    systemPrompt: \"You are a helpful assistant that can check weather and recommend movies.\"\n",
    "});\n",
    "\n",
    "// Use the agent\n",
    "const result = await agent.invoke({\n",
    "    messages: [{ role: \"user\", content: \"What's the weather in NYC? Also recommend some sci-fi movies.\" }]\n",
    "});\n",
    "\n",
    "// Print the conversation\n",
    "for (const msg of result.messages) {\n",
    "    console.log(`[${msg.constructor.name}]:`, msg.content);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ed05b",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "The agent automatically:\n",
    "1. Analyzed the user's request\n",
    "2. Called `get_weather` for NYC\n",
    "3. Called `search_movies` for \"sci-fi\"\n",
    "4. Synthesized the results into a natural response\n",
    "\n",
    "You can visualize the agent's structure (note: visualization requires additional setup):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93949b36",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- `createAgent()` builds a complete agent in ~10 lines\n",
    "- The agent automatically handles the reasoning ‚Üí action ‚Üí observation loop\n",
    "- Built on LangGraph for production features (persistence, streaming, human-in-the-loop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac60248",
   "metadata": {},
   "source": [
    "## Part 5: Adding Memory & State\n",
    "\n",
    "Right now, each agent invocation is independent. Let's add **memory** so the agent can maintain context across multiple interactions.\n",
    "\n",
    "LangGraph uses **checkpointers** to save and restore state:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc3b0a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: Here are some sci-fi movies you might enjoy:\n",
      "\n",
      "1. **Dune**\n",
      "2. **Interstellar**\n",
      "3. **Blade Runner 2049**\n",
      "\n",
      "Let me know if you need more recommendations!\n",
      "\n",
      "Response 2: Your name is Alice, and you love sci-fi movies.\n"
     ]
    }
   ],
   "source": [
    "import { MemorySaver } from \"@langchain/langgraph\";\n",
    "import { v4 as uuidv4 } from \"uuid\";\n",
    "\n",
    "// Create a checkpointer for memory\n",
    "const checkpointer = new MemorySaver();\n",
    "\n",
    "// Create an agent with memory\n",
    "const agentWithMemory = createAgent({\n",
    "    model: \"openai:gpt-4o-mini\",\n",
    "    tools: [getWeather, searchMovies],\n",
    "    systemPrompt: \"You are a helpful assistant.\",\n",
    "    checkpointer: checkpointer\n",
    "});\n",
    "\n",
    "// Create a thread for this conversation\n",
    "const threadId = uuidv4();\n",
    "const config = { configurable: { thread_id: threadId } };\n",
    "\n",
    "// First interaction\n",
    "const result1 = await agentWithMemory.invoke(\n",
    "    { messages: [{ role: \"user\", content: \"My name is Alice and I love sci-fi movies.\" }] },\n",
    "    config\n",
    ");\n",
    "\n",
    "console.log(\"Response 1:\", result1.messages[result1.messages.length - 1].content);\n",
    "\n",
    "// Second interaction - the agent remembers!\n",
    "const result2 = await agentWithMemory.invoke(\n",
    "    { messages: [{ role: \"user\", content: \"What's my name and what movies do I like?\" }] },\n",
    "    config\n",
    ");\n",
    "console.log(\"\\nResponse 2:\", result2.messages[result2.messages.length - 1].content);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d2c9c",
   "metadata": {},
   "source": [
    "### Understanding State & Threads\n",
    "\n",
    "- **State**: The agent's \"memory\" - includes message history and any custom data\n",
    "- **Thread**: A conversation session identified by `thread_id`\n",
    "- **Checkpointer**: Saves state after each step, enabling memory and error recovery\n",
    "\n",
    "Each thread is independent:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2469fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New thread response: I'm sorry, but I don't know your name. Could you please tell me?\n"
     ]
    }
   ],
   "source": [
    "// New thread - agent won't remember Alice\n",
    "const newThreadId = uuidv4();\n",
    "const newConfig = { configurable: { thread_id: newThreadId } };\n",
    "\n",
    "const result3 = await agentWithMemory.invoke(\n",
    "    { messages: [{ role: \"user\", content: \"What's my name?\" }] },\n",
    "    newConfig\n",
    ");\n",
    "console.log(\"New thread response:\", result3.messages[result3.messages.length - 1].content);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0622cc5",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- Checkpointers enable memory across interactions\n",
    "- Thread IDs separate different conversations\n",
    "- State persists automatically - no manual state management needed!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f519b4d5",
   "metadata": {},
   "source": [
    "## Part 6: Streaming for Better UX\n",
    "\n",
    "LLMs can take a while to respond. **Streaming** shows progress in real-time, dramatically improving user experience.\n",
    "\n",
    "LangChain supports multiple streaming modes:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ad592",
   "metadata": {},
   "source": [
    "### Streaming Agent Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aead9eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming agent steps:\n",
      "\n",
      "Step: model_request\n",
      "   Tool call: get_weather\n",
      "\n",
      "Step: tools\n",
      "   Content: {\"temperature_fahrenheit\":46.8,\"weather_code\":3}\n",
      "\n",
      "Step: model_request\n",
      "   Content: The current temperature in Boston is 46.8¬∞F, and the weather is mostly cloudy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Stream agent progress with streamMode=\"updates\"\n",
    "console.log(\"Streaming agent steps:\\n\");\n",
    "\n",
    "for await (const chunk of await agent.stream(\n",
    "    { messages: [{ role: \"user\", content: \"What's the weather in Boston?\" }] },\n",
    "    { streamMode: \"updates\" } as any\n",
    ")) {\n",
    "    for (const [nodeName, data] of Object.entries(chunk)) {\n",
    "        console.log(`Step: ${nodeName}`);\n",
    "        // Type guard to check if data has messages property\n",
    "        if (data && typeof data === \"object\" && \"messages\" in data) {\n",
    "            const messages = data.messages as any[];\n",
    "            const message = messages[messages.length - 1];\n",
    "            if (message.tool_calls && message.tool_calls.length > 0) {\n",
    "                console.log(`   Tool call: ${message.tool_calls[0].name}`);\n",
    "            } else if (message.content) {\n",
    "                const content = message.content.length > 100 \n",
    "                    ? `${message.content.substring(0, 100)}...` \n",
    "                    : message.content;\n",
    "                console.log(`   Content: ${content}`);\n",
    "            }\n",
    "        }\n",
    "        console.log();\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f1fc3",
   "metadata": {},
   "source": [
    "### Streaming LLM Tokens\n",
    "\n",
    "For a ChatGPT-like experience, stream tokens as they're generated:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92c04394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming tokens:\n",
      "\n",
      "LangGraph is a platform that allows users to create and manage language models using graph-based techniques, facilitating better understanding and manipulation of natural language processing tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Stream tokens - simplified ChatGPT-like experience\n",
    "console.log(\"Streaming tokens:\\n\");\n",
    "\n",
    "for await (const [token, metadata] of await agent.stream(\n",
    "    { messages: [{ role: \"user\", content: \"Tell me about LangGraph in one sentence.\" }] },\n",
    "    { streamMode: \"messages\" } as any\n",
    ")) {\n",
    "    // Print any text content we find\n",
    "    if (token.text) {\n",
    "        process.stdout.write(token.text);\n",
    "    }\n",
    "}\n",
    "\n",
    "console.log(\"\\n\");  // New line at the end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543acd63",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- `streamMode: \"updates\"` - See each agent step (useful for debugging)\n",
    "- `streamMode: \"messages\"` - Stream LLM tokens (ChatGPT-like UX)\n",
    "- Streaming is built-in - no extra setup required!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99e4c1",
   "metadata": {},
   "source": [
    "## Part 7: Putting It All Together - A Practical Example\n",
    "\n",
    "Let's build a more realistic agent that combines everything we've learned:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc5e9aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PERSONAL ASSISTANT DEMO\n",
      "==================================================\n",
      "\n",
      "User: Hi, I'm Alice. Can you check my preferences and recommend a movie?\n",
      "\n",
      "Assistant: Hi Alice! Based on your love for sci-fi movies, I recommend checking out **Arrival**, **Ex Machina**, or **The Martian**. These films are sure to engage you with their intriguing stories and visuals. Enjoy your movie time! If there's anything else you'd like, just let me know.\n",
      "\n",
      "User: Also, what's the weather like in San Francisco?\n",
      "\n",
      "Assistant: The weather in San Francisco is currently a pleasant 63.8¬∞F and clear skies. It's a lovely day to explore the city! If you need anything else or have any other questions, feel free to ask!\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "// Create more realistic tools\n",
    "const getUserPreferences = tool(\n",
    "  async ({ userId }: { userId: string }) => {\n",
    "    // Simulate a user database\n",
    "    const preferences: Record<string, string> = {\n",
    "      \"alice\": \"Loves sci-fi movies, prefers warm weather destinations\",\n",
    "      \"bob\": \"Enjoys comedy films, likes cold climates for travel\"\n",
    "    };\n",
    "    return preferences[userId.toLowerCase()] || \"No preferences found\";\n",
    "  },\n",
    "  {\n",
    "    name: \"get_user_preferences\",\n",
    "    description: \"Get a user's saved preferences.\",\n",
    "    schema: z.object({\n",
    "      userId: z.string().describe(\"The user ID to look up\")\n",
    "    })\n",
    "  }\n",
    ");\n",
    "\n",
    "const bookRecommendation = tool(\n",
    "  async ({ genre, userPreferences }: { genre: string; userPreferences?: string }) => {\n",
    "    const recommendations: Record<string, string> = {\n",
    "      \"sci-fi\": \"Based on your preferences, try: Arrival, Ex Machina, or The Martian\",\n",
    "      \"comedy\": \"Based on your preferences, try: The Big Lebowski, Anchorman, or Bridesmaids\"\n",
    "    };\n",
    "    return recommendations[genre.toLowerCase()] || \"No recommendations available\";\n",
    "  },\n",
    "  {\n",
    "    name: \"book_recommendation\",\n",
    "    description: \"Get personalized movie recommendations based on genre and user preferences.\",\n",
    "    schema: z.object({\n",
    "      genre: z.string().describe(\"The movie genre\"),\n",
    "      userPreferences: z.string().optional().describe(\"Optional user preferences\")\n",
    "    })\n",
    "  }\n",
    ");\n",
    "\n",
    "// Create a helpful assistant agent\n",
    "const assistant = createAgent({\n",
    "    model: \"openai:gpt-4o-mini\",\n",
    "    tools: [getWeather, getUserPreferences, bookRecommendation],\n",
    "    systemPrompt: `You are a helpful personal assistant. \n",
    "    \n",
    "    You can:\n",
    "    - Check weather for any city\n",
    "    - Look up user preferences\n",
    "    - Recommend movies based on preferences\n",
    "    \n",
    "    Always be friendly and personalize your responses based on user preferences.`,\n",
    "    checkpointer: new MemorySaver()\n",
    "});\n",
    "\n",
    "// Demo conversation\n",
    "const demoThreadId = uuidv4();\n",
    "const demoConfig = { configurable: { thread_id: demoThreadId } };\n",
    "\n",
    "console.log(\"=\".repeat(50));\n",
    "console.log(\"PERSONAL ASSISTANT DEMO\");\n",
    "console.log(\"=\".repeat(50) + \"\\n\");\n",
    "\n",
    "// Interaction 1\n",
    "console.log(\"User: Hi, I'm Alice. Can you check my preferences and recommend a movie?\\n\");\n",
    "const demoResult1 = await assistant.invoke(\n",
    "    { messages: [{ role: \"user\", content: \"Hi, I'm Alice. Can you check my preferences and recommend a movie?\" }] },\n",
    "    demoConfig\n",
    ");\n",
    "console.log(`Assistant: ${demoResult1.messages[demoResult1.messages.length - 1].content}\\n`);\n",
    "\n",
    "// Interaction 2\n",
    "console.log(\"User: Also, what's the weather like in San Francisco?\\n\");\n",
    "const demoResult2 = await assistant.invoke(\n",
    "    { messages: [{ role: \"user\", content: \"Also, what's the weather like in San Francisco?\" }] },\n",
    "    demoConfig\n",
    ");\n",
    "console.log(`Assistant: ${demoResult2.messages[demoResult2.messages.length - 1].content}\\n`);\n",
    "\n",
    "console.log(\"=\".repeat(50));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e4c440",
   "metadata": {},
   "source": [
    "## Part 8: Next Steps - Exploring LangGraph Primitives\n",
    "\n",
    "We've been using `createAgent()`, which is built on **LangGraph**. LangGraph gives you full control over agent behavior using three core primitives:\n",
    "\n",
    "### Core LangGraph Concepts:\n",
    "\n",
    "1. **State**\n",
    "   - Shared data structure passed between nodes\n",
    "   - Represents the agent's \"memory\"\n",
    "   - Can include messages, custom data, etc.\n",
    "\n",
    "2. **Nodes**\n",
    "   - Functions that process state\n",
    "   - Each node performs a specific task\n",
    "   - Examples: call LLM, execute tool, validate input\n",
    "\n",
    "3. **Edges**\n",
    "   - Define flow between nodes\n",
    "   - Can be normal (always go to next node)\n",
    "   - Or conditional (decide based on logic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b5a9a",
   "metadata": {},
   "source": [
    "### When to use `createAgent()` vs custom LangGraph?\n",
    "\n",
    "**Use `createAgent()` when:**\n",
    "- Building standard ReAct-style agents\n",
    "- You need quick prototyping\n",
    "- Default behavior works for your use case\n",
    "\n",
    "**Use custom LangGraph when:**\n",
    "- You need custom control flow (e.g., approval workflows)\n",
    "- Building multi-agent systems\n",
    "- Implementing human-in-the-loop patterns\n",
    "- Complex state management requirements\n",
    "\n",
    "For more advanced patterns, check out:\n",
    "- [LangGraph Documentation](https://docs.langchain.com/oss/javascript/langgraph/overview)\n",
    "- [LangChain Academy](https://academy.langchain.com/)\n",
    "- The `multi_agent.ipynb` notebook in this repo (LangGraph 201)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4bfcb9",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've learned the core concepts of building agents with LangChain and LangGraph:\n",
    "\n",
    "‚úÖ **Models** - Standardized interface across providers  \n",
    "‚úÖ **Messages** - Building block of conversations  \n",
    "‚úÖ **Tools** - Extending LLM capabilities  \n",
    "‚úÖ **Agents** - Automated reasoning and action loops  \n",
    "‚úÖ **Memory** - Maintaining context across interactions  \n",
    "‚úÖ **Streaming** - Real-time user experience  \n",
    "‚úÖ **LangGraph** - The foundation powering it all\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "1. **Build your own agent** with your specific tools and use case\n",
    "2. **Explore advanced patterns** in the `multi_agent.ipynb` notebook\n",
    "3. **Add debugging** with [LangSmith](https://smith.langchain.com)\n",
    "4. **Deploy to production** using LangGraph's persistence and error recovery\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [LangChain Documentation](https://docs.langchain.com/oss/javascript/langchain/overview)\n",
    "- [LangGraph Documentation](https://docs.langchain.com/oss/javascript/langgraph/overview)\n",
    "- [LangSmith for Debugging](https://smith.langchain.com)\n",
    "- [LangChain Academy](https://academy.langchain.com/)\n",
    "<br>\n",
    "<br>\n",
    "---\n",
    "<br>\n",
    "\n",
    "**Happy building!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
